{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aecf68-2375-45df-be7d-e3b43d7638dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "df=pd.read_csv('/mnt/data/spam.csv',encoding='latin-1')[['v1','v2']].rename(columns={'v1':'label','v2':'text'})\n",
    "df['label']=df['label'].map({'spam':1,'ham':0})\n",
    "sw=set(stopwords.words('english'))\n",
    "def clean(t):\n",
    "    t=str(t).lower()\n",
    "    t=''.join(ch for ch in t if ch not in string.punctuation)\n",
    "    return ' '.join(w for w in t.split() if w not in sw)\n",
    "df['text']=df['text'].apply(clean)\n",
    "X=df['text'].values\n",
    "y=df['label'].values\n",
    "tf=TfidfVectorizer()\n",
    "Xv=tf.fit_transform(X)\n",
    "Xtr,Xte,ytr,yte=train_test_split(Xv,y,test_size=0.2,random_state=42,stratify=y)\n",
    "print(\"Class distribution (train):\",np.bincount(ytr))\n",
    "stump=DecisionTreeClassifier(max_depth=1,random_state=42)\n",
    "stump.fit(Xtr,ytr)\n",
    "p_tr=stump.predict(Xtr)\n",
    "p_te=stump.predict(Xte)\n",
    "print(\"Stump Train Acc\",accuracy_score(ytr,p_tr),\"Test Acc\",accuracy_score(yte,p_te))\n",
    "print(\"Stump Confusion Matrix\\n\",confusion_matrix(yte,p_te))\n",
    "print(\"Stump Classification Report\\n\",classification_report(yte,p_te))\n",
    "T=15\n",
    "n=Xtr.shape[0]\n",
    "w=np.ones(n)/n\n",
    "train_errors=[]\n",
    "alphas=[]\n",
    "models=[]\n",
    "for t in range(1,T+1):\n",
    "    clf=DecisionTreeClassifier(max_depth=1,random_state=42)\n",
    "    clf.fit(Xtr,ytr,sample_weight=w)\n",
    "    pred=clf.predict(Xtr)\n",
    "    miss=(pred!=ytr).astype(int)\n",
    "    err=np.dot(w,miss)/w.sum()\n",
    "    if err==0:\n",
    "        alpha=0.5*np.log((1-1e-12)/(1e-12))\n",
    "    else:\n",
    "        alpha=0.5*np.log((1-err)/max(err,1e-12))\n",
    "    w=w*np.exp(alpha*miss*2- alpha*(1-miss)*2) # alternative update to emphasize misclassified\n",
    "    w=w/w.sum()\n",
    "    models.append((clf,alpha))\n",
    "    train_pred=np.sign(sum(a* (m.predict(Xtr)*2-1) for m,a in models))\n",
    "    train_pred=(train_pred+1)//2\n",
    "    train_acc=accuracy_score(ytr,train_pred)\n",
    "    train_errors.append(err)\n",
    "    alphas.append(alpha)\n",
    "    mis_idx=np.where(miss==1)[0]\n",
    "    print(\"Iteration\",t)\n",
    "    print(\"Misclassified indices (sampled subset)\",mis_idx[:20])\n",
    "    print(\"Weights of misclassified (sampled subset)\",np.round(w[mis_idx[:20]],6))\n",
    "    print(\"Alpha\",alpha)\n",
    "plt.figure()\n",
    "plt.plot(range(1,T+1),train_errors,marker='o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Weighted error')\n",
    "plt.title('Iteration vs weighted error')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(range(1,T+1),alphas,marker='o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Alpha')\n",
    "plt.title('Iteration vs alpha')\n",
    "plt.show()\n",
    "def predict_ensemble(models,X):\n",
    "    s=sum(a*(m.predict(X)*2-1) for m,a in models)\n",
    "    return ((np.sign(s)+1)//2).astype(int)\n",
    "train_final=predict_ensemble(models,Xtr)\n",
    "test_final=predict_ensemble(models,Xte)\n",
    "print(\"Manual AdaBoost Train Acc\",accuracy_score(ytr,train_final),\"Test Acc\",accuracy_score(yte,test_final))\n",
    "print(\"Manual AdaBoost Confusion Matrix\\n\",confusion_matrix(yte,test_final))\n",
    "skada=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),n_estimators=100,learning_rate=0.6,random_state=42)\n",
    "skada.fit(Xtr,ytr)\n",
    "print(\"Sklearn AdaBoost Train\",skada.score(Xtr,ytr),\"Test\",skada.score(Xte,yte))\n",
    "print(\"Sklearn Confusion Matrix\\n\",confusion_matrix(yte,skada.predict(Xte)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb6404-e0d9-4f92-ab8a-65d18bfd97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "import sklearn\n",
    "from sklearn.datasets import load_heart_disease\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "hd=load_heart_disease()\n",
    "X=pd.DataFrame(hd.data,columns=hd.feature_names)\n",
    "y=hd.target\n",
    "cat_cols=[c for c in X.columns if X[c].nunique()<=6]\n",
    "num_cols=[c for c in X.columns if c not in cat_cols]\n",
    "ct=ColumnTransformer([('o',OneHotEncoder(handle_unknown='ignore'),cat_cols),\n",
    "                      ('s',StandardScaler(),num_cols)])\n",
    "Xt=ct.fit_transform(X)\n",
    "Xtr,Xte,ytr,yte=train_test_split(Xt,y,test_size=0.2,random_state=42,stratify=y)\n",
    "stump=DecisionTreeClassifier(max_depth=1,random_state=42)\n",
    "stump.fit(Xtr,ytr)\n",
    "print(\"Heart Stump Train\",stump.score(Xtr,ytr),\"Test\",stump.score(Xte,yte))\n",
    "print(\"Heart Stump CM\\n\",confusion_matrix(yte,stump.predict(Xte)))\n",
    "print(\"Heart Stump Report\\n\",classification_report(yte,stump.predict(Xte)))\n",
    "n_estimators=[5,10,25,50,100]\n",
    "learning_rate=[0.1,0.5,1.0]\n",
    "results=[]\n",
    "for lr in learning_rate:\n",
    "    accs=[]\n",
    "    for n_est in n_estimators:\n",
    "        m=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),n_estimators=n_est,learning_rate=lr,random_state=42)\n",
    "        m.fit(Xtr,ytr)\n",
    "        accs.append(m.score(Xte,yte))\n",
    "        results.append((lr,n_est,m.score(Xte,yte),m))\n",
    "    plt.plot(n_estimators,accs,label=f'lr={lr}')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.title('n_estimators vs accuracy for different learning rates')\n",
    "plt.show()\n",
    "best=max(results,key=lambda x:x[2])\n",
    "print(\"Best config lr,n_est,acc\",best[0],best[1],best[2])\n",
    "best_model=best[3]\n",
    "errors=[]\n",
    "weights_history=[]\n",
    "for est in best_model.estimators_:\n",
    "    pred=est.predict(Xtr)\n",
    "    miss=(pred!=ytr).astype(int)\n",
    "    err=miss.mean()\n",
    "    errors.append(err)\n",
    "weights_history=best_model.estimator_weights_\n",
    "plt.figure()\n",
    "plt.plot(range(1,len(errors)+1),errors,marker='o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Weak learner error')\n",
    "plt.title('Weak learner error vs iteration')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.hist(best_model.estimator_weights_,bins=20)\n",
    "plt.xlabel('Final estimator weights')\n",
    "plt.title('Distribution of estimator weights')\n",
    "plt.show()\n",
    "importances=best_model.feature_importances_\n",
    "if hasattr(Xt,'toarray'):\n",
    "    feat_names=ct.get_feature_names_out()\n",
    "else:\n",
    "    feat_names=ct.get_feature_names_out()\n",
    "imp_df=pd.Series(importances,index=feat_names).sort_values(ascending=False)\n",
    "print(\"Top 5 features\\n\",imp_df.head(5))\n",
    "print(\"Feature importances explanation: top features likely relate to known medical risk factors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2fc593-2fdd-4ee2-bba7-42137a237541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "import os\n",
    "path='/mnt/data/WISDM_ar_v1.1_raw.txt'\n",
    "rows=[]\n",
    "with open(path,'r') as f:\n",
    "    for line in f:\n",
    "        parts=line.strip().split(',')\n",
    "        if len(parts)>=5:\n",
    "            user,activity,timestamp,x,y,z=parts[0],parts[1],parts[2],parts[3],parts[4]\n",
    "            try:\n",
    "                rows.append([user,activity,float(timestamp),float(x),float(y),float(z)])\n",
    "            except:\n",
    "                continue\n",
    "wdf=pd.DataFrame(rows,columns=['user','activity','timestamp','x','y','z'])\n",
    "vig=set(['Jogging','Upstairs','UP','jogging','upstairs'])\n",
    "light=set(['Walking','Sitting','Standing','Downstairs','walking','sitting','standing','downstairs'])\n",
    "def label(a):\n",
    "    if any(v.lower() in a.lower() for v in ['jog','up']):\n",
    "        return 1\n",
    "    return 0\n",
    "wdf['label']=wdf['activity'].apply(label)\n",
    "wdf=wdf.dropna()\n",
    "X=wdf[['x','y','z']].values\n",
    "y=wdf['label'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtr,Xte,ytr,yte=train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "Xtr_s=sc.fit_transform(Xtr)\n",
    "Xte_s=sc.transform(Xte)\n",
    "stump=DecisionTreeClassifier(max_depth=1,random_state=42)\n",
    "stump.fit(Xtr_s,ytr)\n",
    "print(\"WISDM Stump Train\",stump.score(Xtr_s,ytr),\"Test\",stump.score(Xte_s,yte))\n",
    "T=20\n",
    "n=Xtr_s.shape[0]\n",
    "w=np.ones(n)/n\n",
    "models=[]\n",
    "alphas=[]\n",
    "errors=[]\n",
    "for t in range(1,T+1):\n",
    "    clf=DecisionTreeClassifier(max_depth=1,random_state=42)\n",
    "    clf.fit(Xtr_s,ytr,sample_weight=w)\n",
    "    pred=clf.predict(Xtr_s)\n",
    "    miss=(pred!=ytr).astype(int)\n",
    "    err=np.dot(w,miss)/w.sum()\n",
    "    if err==0:\n",
    "        alpha=0.5*np.log((1-1e-12)/(1e-12))\n",
    "    else:\n",
    "        alpha=0.5*np.log((1-err)/max(err,1e-12))\n",
    "    w=w*np.exp(alpha*miss*2- alpha*(1-miss)*2)\n",
    "    w=w/w.sum()\n",
    "    models.append((clf,alpha))\n",
    "    alphas.append(alpha)\n",
    "    errors.append(err)\n",
    "    print(\"Iteration\",t,\"Mis idx\",np.where(miss==1)[0][:20],\"Weights sample\",np.round(w[np.where(miss==1)[0][:20]],6))\n",
    "plt.figure()\n",
    "plt.plot(range(1,T+1),errors,marker='o')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Weighted error')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(range(1,T+1),alphas,marker='o')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Alpha')\n",
    "plt.show()\n",
    "def ens_predict(models,X):\n",
    "    s=sum(a*(m.predict(X)*2-1) for m,a in models)\n",
    "    return ((np.sign(s)+1)//2).astype(int)\n",
    "train_pred=ens_predict(models,Xtr_s)\n",
    "test_pred=ens_predict(models,Xte_s)\n",
    "print(\"Manual AdaBoost WISDM Train\",accuracy_score(ytr,train_pred),\"Test\",accuracy_score(yte,test_pred))\n",
    "print(\"Confusion Matrix\\n\",confusion_matrix(yte,test_pred))\n",
    "skada=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),n_estimators=100,learning_rate=1.0,random_state=42)\n",
    "skada.fit(Xtr_s,ytr)\n",
    "print(\"Sklearn AdaBoost WISDM Train\",skada.score(Xtr_s,ytr),\"Test\",skada.score(Xte_s,yte))\n",
    "print(\"Sklearn Conf Matrix\\n\",confusion_matrix(yte,skada.predict(Xte_s)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
